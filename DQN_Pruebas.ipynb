{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9105507/Trabajo_Drones_2023_2024/blob/main/DQN_Pruebas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Train a Deep Q Network with TF-Agents\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "If you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet\n",
        "!pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX0aSKBCYmj2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import numpy as np\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.trajectories import time_step as ts  # Importar ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "outputs": [],
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NspmzG4nP3b9"
      },
      "outputs": [],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "outputs": [],
      "source": [
        "# env_name = 'CartPole-v0'\n",
        "# env = suite_gym.load(env_name)\n",
        "\n",
        "# env.reset()\n",
        "# PIL.Image.fromarray(env.render())\n",
        "\n",
        "# print('Observation Spec:')\n",
        "# print(env.time_step_spec().observation)\n",
        "\n",
        "# print('Reward Spec:')\n",
        "# print(env.time_step_spec().reward)\n",
        "\n",
        "# print('Action Spec:')\n",
        "# print(env.action_spec())\n",
        "\n",
        "# train_py_env = suite_gym.load(env_name)\n",
        "# eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "# train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomEnv(py_environment.PyEnvironment):\n",
        "#     def __init__(self):\n",
        "#         super(CustomEnv, self).__init__()\n",
        "\n",
        "#         # Definir el espacio de observación y el espacio de acción como TensorSpec\n",
        "#         self._observation_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=9, name='observation')\n",
        "#         self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "\n",
        "#         # Inicializar el estado actual\n",
        "#         self._state = None\n",
        "\n",
        "#     def action_spec(self):\n",
        "#         return self._action_spec\n",
        "\n",
        "#     def observation_spec(self):\n",
        "#         return self._observation_spec\n",
        "\n",
        "#     def _reset(self):\n",
        "#         # Restablecer el estado inicial aleatorio\n",
        "#         self._state = np.random.randint(10)\n",
        "#         return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "#     def _step(self, action):\n",
        "#         # Verificar si el estado actual está definido\n",
        "#         if self._state is None:\n",
        "#             raise ValueError(\"El estado actual no está definido. Debe llamar a reset() primero.\")\n",
        "\n",
        "#         # Ejecutar la acción y actualizar el estado\n",
        "#         self._state += action\n",
        "\n",
        "#         # Calcular la recompensa\n",
        "#         reward = 1 if self._state % 2 == 0 else -1\n",
        "\n",
        "#         # Verificar si se ha alcanzado un estado final\n",
        "#         done = self._state >= 10\n",
        "\n",
        "#         # Devolver la observación, la recompensa, si el episodio ha terminado y la información adicional\n",
        "#         return ts.transition(np.array([self._state], dtype=np.int32), reward=reward, discount=1.0 if done else 0.0)\n",
        "\n",
        "#     def render(self, mode='human'):\n",
        "#         # Renderizar el estado actual (opcional)\n",
        "#         print(f\"Estado actual: {self._state}\")\n",
        "\n",
        "#     def close(self):\n",
        "#         pass\n",
        "\n",
        "class CardGameEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Make sure episodes don't go on forever.\n",
        "    if action == 1:\n",
        "      self._episode_ended = True\n",
        "    elif action == 0:\n",
        "      new_card = np.random.randint(1, 11)\n",
        "      self._state += new_card\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "    if self._episode_ended or self._state >= 21:\n",
        "      reward = self._state - 21 if self._state <= 21 else -21\n",
        "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)\n",
        "\n",
        "# Crear una instancia del entorno personalizado\n",
        "env = CardGameEnv()\n",
        "\n",
        "train_py_env = CardGameEnv()\n",
        "eval_py_env = CardGameEnv()\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "metadata": {
        "id": "qQs3XT8smhwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "\n",
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    CardGameEnv())\n",
        "\n",
        "time_step = example_environment.reset()\n",
        "\n",
        "random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "    total_return = 0.0\n",
        "    max_steps_per_episode = 1000  # Definir el límite máximo de pasos por episodio\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_return = 0.0\n",
        "        episode_step = 0  # Contador de pasos en el episodio\n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step)\n",
        "            time_step = environment.step(action_step.action)\n",
        "            episode_return += time_step.reward\n",
        "            episode_step += 1\n",
        "\n",
        "            if episode_step >= max_steps_per_episode:\n",
        "                break  # Salir del bucle si se supera el límite de pasos\n",
        "\n",
        "        total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n",
        "\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IZ-3HcqgE1z"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy6g1tGcfRlw"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr1KSAEGG4h9"
      },
      "outputs": [],
      "source": [
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wZnLu2ViO4E"
      },
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "outputs": [],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K13AST-2ppOq"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th5w5Sff0b16"
      },
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data\n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DIVERGE, NO FUNCIONA**\n",
        "**CAMBIAR DE EJEMPLO DE ENTORNO, A OTRO, EL DE LA DUCHA POR EJEMPLO (VÍDEO YOUTUBE)**"
      ],
      "metadata": {
        "id": "JH_ocwiHxlwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Todo el código junto"
      ],
      "metadata": {
        "id": "_cxV3IfZ89Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #ENTORNO\n",
        "# class ShowerEnv(py_environment.PyEnvironment):\n",
        "#     def __init__(self):\n",
        "#         # Actions we can take, down, stay, up\n",
        "#         self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "#         # Temperature array\n",
        "#         self._observation_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=np.float32, minimum=0, maximum=100, name='observation')\n",
        "#         # Set start temp\n",
        "#         self._state = 38 + random.randint(-3, 3)\n",
        "#         # Set shower length\n",
        "#         self._shower_length = 60\n",
        "#         # Initialize current time step\n",
        "#         self._current_time_step = None\n",
        "\n",
        "#     def action_spec(self):\n",
        "#         return self._action_spec\n",
        "\n",
        "#     def observation_spec(self):\n",
        "#         return self._observation_spec\n",
        "\n",
        "#     def _reset(self):\n",
        "#         # Reset shower temperature\n",
        "#         self._state = 38 + random.randint(-3, 3)\n",
        "#         # Reset shower time\n",
        "#         self._shower_length = 60\n",
        "#         self._current_time_step = time_step.restart(np.array([self._state], dtype=np.float32))\n",
        "#         return self._current_time_step\n",
        "\n",
        "#     def _step(self, action):\n",
        "#         if self._current_time_step is None:\n",
        "#             return self.reset()\n",
        "\n",
        "#         # Apply action\n",
        "#         self._state += action - 1\n",
        "#         # Reduce shower length by 1 second\n",
        "#         self._shower_length -= 1\n",
        "\n",
        "#         # Calculate reward\n",
        "#         if 37 <= self._state <= 39:\n",
        "#             reward = 1\n",
        "#         else:\n",
        "#             reward = -1\n",
        "\n",
        "#         # Check if shower is done\n",
        "#         if self._shower_length <= 0:\n",
        "#             done = True\n",
        "#         else:\n",
        "#             done = False\n",
        "\n",
        "#         # Apply temperature noise\n",
        "#         # self._state += random.randint(-1, 1)\n",
        "\n",
        "#         # Set placeholder for info\n",
        "#         info = {}\n",
        "\n",
        "#         self._current_time_step = time_step.transition(\n",
        "#             np.array([self._state], dtype=np.float32),\n",
        "#             reward=reward,\n",
        "#             discount=1.0 if done else 0.0)\n",
        "\n",
        "#         return self._current_time_step\n",
        "\n",
        "#     def render(self):\n",
        "#         # Implement viz\n",
        "#         pass\n",
        "\n",
        "# # Crear una instancia del entorno personalizado\n",
        "# env = ShowerEnv()\n",
        "\n",
        "# train_py_env = ShowerEnv()\n",
        "# eval_py_env = ShowerEnv()\n",
        "\n",
        "# train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "# #AGENTE\n",
        "# fc_layer_params = (100, 50)\n",
        "# action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "# num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# # Define a helper function to create Dense layers configured with the right\n",
        "# # activation and kernel initializer.\n",
        "# def dense_layer(num_units):\n",
        "#   return tf.keras.layers.Dense(\n",
        "#       num_units,\n",
        "#       activation=tf.keras.activations.relu,\n",
        "#       kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "#           scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# # QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# # with `num_actions` units to generate one q_value per available action as\n",
        "# # its output.\n",
        "# dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "# q_values_layer = tf.keras.layers.Dense(\n",
        "#     num_actions,\n",
        "#     activation=None,\n",
        "#     kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "#         minval=-0.03, maxval=0.03),\n",
        "#     bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "# q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# train_step_counter = tf.Variable(0)\n",
        "\n",
        "# agent = dqn_agent.DqnAgent(\n",
        "#     train_env.time_step_spec(),\n",
        "#     train_env.action_spec(),\n",
        "#     q_network=q_net,\n",
        "#     optimizer=optimizer,\n",
        "#     td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "#     train_step_counter=train_step_counter)\n",
        "\n",
        "# agent.initialize()\n",
        "\n",
        "# #POLICY\n",
        "# eval_policy = agent.policy\n",
        "# collect_policy = agent.collect_policy\n",
        "\n",
        "# random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "#                                                 train_env.action_spec())\n",
        "\n",
        "# example_environment = tf_py_environment.TFPyEnvironment(\n",
        "#     ShowerEnv())\n",
        "\n",
        "# time_step = example_environment.reset()\n",
        "\n",
        "# random_policy.action(time_step)\n",
        "\n",
        "# #METRICS\n",
        "# def compute_avg_return(environment, policy, num_episodes=10):\n",
        "#     total_return = 0.0\n",
        "#     max_steps_per_episode = 1000  # Definir el límite máximo de pasos por episodio\n",
        "\n",
        "#     for _ in range(num_episodes):\n",
        "#         time_step = environment.reset()\n",
        "#         episode_return = 0.0\n",
        "#         episode_step = 0  # Contador de pasos en el episodio\n",
        "\n",
        "#         while not time_step.is_last():\n",
        "#             action_step = policy.action(time_step)\n",
        "#             time_step = environment.step(action_step.action)\n",
        "#             episode_return += time_step.reward\n",
        "#             episode_step += 1\n",
        "\n",
        "#             if episode_step >= max_steps_per_episode:\n",
        "#                 break  # Salir del bucle si se supera el límite de pasos\n",
        "\n",
        "#         total_return += episode_return\n",
        "\n",
        "#     avg_return = total_return / num_episodes\n",
        "#     return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# # See also the metrics module for standard implementations of different metrics.\n",
        "# # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n",
        "\n",
        "# compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "\n",
        "# #REPLAY BUFFER\n",
        "# table_name = 'uniform_table'\n",
        "# replay_buffer_signature = tensor_spec.from_spec(\n",
        "#       agent.collect_data_spec)\n",
        "# replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "#     replay_buffer_signature)\n",
        "\n",
        "# table = reverb.Table(\n",
        "#     table_name,\n",
        "#     max_size=replay_buffer_max_length,\n",
        "#     sampler=reverb.selectors.Uniform(),\n",
        "#     remover=reverb.selectors.Fifo(),\n",
        "#     rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "#     signature=replay_buffer_signature)\n",
        "\n",
        "# reverb_server = reverb.Server([table])\n",
        "\n",
        "# replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "#     agent.collect_data_spec,\n",
        "#     table_name=table_name,\n",
        "#     sequence_length=2,\n",
        "#     local_server=reverb_server)\n",
        "\n",
        "# rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "#   replay_buffer.py_client,\n",
        "#   table_name,\n",
        "#   sequence_length=2)\n",
        "\n",
        "# #DATA COLLECTION\n",
        "# py_driver.PyDriver(\n",
        "#     env,\n",
        "#     py_tf_eager_policy.PyTFEagerPolicy(\n",
        "#       random_policy, use_tf_function=True),\n",
        "#     [rb_observer],\n",
        "#     max_steps=initial_collect_steps).run(train_py_env.reset())\n",
        "\n",
        "# # Dataset generates trajectories with shape [Bx2x...]\n",
        "# dataset = replay_buffer.as_dataset(\n",
        "#     num_parallel_calls=3,\n",
        "#     sample_batch_size=batch_size,\n",
        "#     num_steps=2).prefetch(3)\n",
        "\n",
        "# dataset\n",
        "\n",
        "# iterator = iter(dataset)\n",
        "# print(iterator)\n",
        "\n",
        "# #TRAINING\n",
        "# try:\n",
        "#   %%time\n",
        "# except:\n",
        "#   pass\n",
        "\n",
        "# # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "# agent.train = common.function(agent.train)\n",
        "\n",
        "# # Reset the train step.\n",
        "# agent.train_step_counter.assign(0)\n",
        "\n",
        "# # Evaluate the agent's policy once before training.\n",
        "# avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "# returns = [avg_return]\n",
        "\n",
        "# # Reset the environment.\n",
        "# time_step = train_py_env.reset()\n",
        "\n",
        "# # Create a driver to collect experience.\n",
        "# collect_driver = py_driver.PyDriver(\n",
        "#     env,\n",
        "#     py_tf_eager_policy.PyTFEagerPolicy(\n",
        "#       agent.collect_policy, use_tf_function=True),\n",
        "#     [rb_observer],\n",
        "#     max_steps=collect_steps_per_iteration)\n",
        "\n",
        "# for _ in range(num_iterations):\n",
        "\n",
        "#   # Collect a few steps and save to the replay buffer.\n",
        "#   time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "#   # Sample a batch of data from the buffer and update the agent's network.\n",
        "#   experience, unused_info = next(iterator)\n",
        "#   train_loss = agent.train(experience).loss\n",
        "\n",
        "#   step = agent.train_step_counter.numpy()\n",
        "\n",
        "#   if step % log_interval == 0:\n",
        "#     print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "#   if step % eval_interval == 0:\n",
        "#     avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "#     print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "#     returns.append(avg_return)"
      ],
      "metadata": {
        "id": "59Jjudwi8_Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.utils import common\n",
        "import numpy as np\n",
        "import imageio\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Definir el entorno personalizado\n",
        "class CardGameEnv(py_environment.PyEnvironment):\n",
        "    def _init_(self):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(1,), dtype=np.int32, minimum=0, maximum=10, name='observation')\n",
        "        self._state = 0\n",
        "        self._episode_ended = False\n",
        "        self._current_time_step = None\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self._state = 0\n",
        "        self._episode_ended = False\n",
        "        self._current_time_step = ts.restart(np.array([self._state], dtype=np.int32))\n",
        "        return self._current_time_step\n",
        "\n",
        "    def _step(self, action):\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        if action == 1:\n",
        "            self._episode_ended = True\n",
        "        elif action == 0:\n",
        "            new_card = np.random.randint(1, 11)\n",
        "            self._state += new_card\n",
        "        else:\n",
        "            raise ValueError('action should be 0 or 1.')\n",
        "\n",
        "        if self._episode_ended or self._state >= 21:\n",
        "            reward = 1 if self._state == 21 else -1\n",
        "            self._current_time_step = ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
        "        else:\n",
        "            reward = 0\n",
        "            self._current_time_step = ts.transition(np.array([self._state], dtype=np.int32), reward=reward)\n",
        "\n",
        "        return self._current_time_step\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"State: {self._state}\")\n",
        "\n",
        "# Inicializar el entorno\n",
        "train_py_env = CardGameEnv()\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_py_env = CardGameEnv()\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "# Definir la red Q\n",
        "fc_layer_params = (100,)\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "# Definir el optimizador\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
        "\n",
        "# Definir el contador de pasos de entrenamiento\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "# Inicializar el agente DQN\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()\n",
        "\n",
        "# Definir las políticas\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "\n",
        "example_environment = tf_py_environment.TFPyEnvironment(CardGameEnv())\n",
        "\n",
        "time_step = example_environment.reset()\n",
        "\n",
        "random_policy.action(time_step)\n",
        "\n",
        "# Recopilación de datos inicial\n",
        "initial_collect_steps = 1000\n",
        "rb_observer = None  # Este debería ser tu observador de buffer de repetición\n",
        "\n",
        "py_driver.PyDriver(\n",
        "    train_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "        random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_env.reset())"
      ],
      "metadata": {
        "id": "4EutPAdyeFRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment.\n",
        "\n",
        "First, create a function to embed videos in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "outputs": [],
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "povaAOcZygLw"
      },
      "source": [
        "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "outputs": [],
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}